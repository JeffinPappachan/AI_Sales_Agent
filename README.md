# AI Sales Agent Backend ğŸ¤–ğŸ“ˆ

An AI-powered Sales Agent backend that interacts with potential customers, understands their intent, maintains conversation context, and assists sales teams by suggesting next best actions and lead scores.

This project was built as part of an **AI Developer Intern Evaluation Task** and demonstrates practical usage of **LLMs, backend APIs, prompt engineering, and business logic design**.

---

## ğŸš€ Features

- ğŸ’¬ Chat API for interacting with sales leads
- ğŸ§  Intent detection (pricing, demo, features, follow-up, not interested)
- ğŸ—‚ï¸ Conversation memory per lead (context-aware replies)
- ğŸ¤– AI-generated responses using a **local LLM (Ollama)**
- ğŸ“Š Lead scoring (Hot / Warm / Cold)
- ğŸ¯ Next best action suggestion for sales representatives
- ğŸ”’ Privacy-first design (no cloud LLM dependency)

---

## ğŸ§  System Architecture

Client (Postman / UI)
â†“
FastAPI Backend
â†“
Intent Detection (Rule-based)
â†“
Conversation Memory (SQLite)
â†“
LLM Response Generation (Ollama)
â†“
Lead Scoring + Next Action
â†“
Structured JSON Response


---

## ğŸ› ï¸ Tech Stack

- **Backend:** FastAPI
- **LLM:** Ollama (llama3.2:1b)
- **Database:** SQLite
- **Language:** Python 3.11+
- **API Testing:** Swagger UI / PowerShell

---

## ğŸ“‚ Project Structure

ai-sales-agent/
â”‚
â”œâ”€â”€ app/
â”‚ â”œâ”€â”€ init.py
â”‚ â”œâ”€â”€ main.py # FastAPI entry point
â”‚ â”œâ”€â”€ schemas.py # API request/response models
â”‚ â”œâ”€â”€ intents.py # Intent detection logic
â”‚ â”œâ”€â”€ memory.py # Conversation storage (SQLite)
â”‚ â”œâ”€â”€ llm.py # Ollama LLM integration
â”‚ â””â”€â”€ scoring.py # Lead scoring logic
â”‚
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md


---

## ğŸ§© Intent Detection

The system detects the following intents:

- `pricing_inquiry`
- `demo_request`
- `feature_inquiry`
- `follow_up`
- `not_interested`

### Design Choice
Intent detection is implemented using **rule-based logic** for:
- Reliability
- Speed
- Deterministic behavior

The LLM is used **only for response generation**, not decision-making.

---

## ğŸ§  LLM Usage & Prompt Design

- Model: `llama3.2:1b` (via Ollama)
- Runs fully **locally**
- Optimized prompt for small models
- Guardrails:
  - Polite
  - Concise
  - Sales-oriented
  - 1â€“2 sentence replies

### Why Ollama?
- No API cost
- No data leakage
- Offline support
- Demonstrates real-world deployment awareness

---

## ğŸ“Š Lead Scoring Logic

Leads are scored from **0â€“100** based on:
- Number of messages
- Detected intent
- Engagement level

| Score Range | Status |
|-----------|--------|
| 70â€“100 | Hot |
| 40â€“69 | Warm |
| 0â€“39 | Cold |

---

## ğŸ”Œ API Specification

### POST `/chat`

#### Request
```json
{
  "lead_id": "123",
  "message": "What is your pricing?"
}

{
  "reply": "Our pricing starts from â‚¹999 per month. Would you like a demo?",
  "intent": "pricing_inquiry",
  "confidence": 0.9,
  "next_action": "offer_demo",
  "lead_score": 60,
  "lead_status": "warm"
}
```
---
## â–¶ï¸ How to Run the Project

### 1ï¸âƒ£ Clone the Repository
```bash
git clone <repo-url>
cd ai-sales-agent
```
### Create Virtual Environment
python -m venv myvenv
myvenv\Scripts\activate   # Windows

### Install Dependencies
```bash
pip install -r requirements.txt
```
### Start Ollama
```bash
ollama run llama3.2:1b
```
Keep this running in the background.

### Start FastAPI Serve
```bash
uvicorn app.main:app --reload
```

### Testing the API
```json
http://127.0.0.1:8000/docs

Invoke-RestMethod -Uri "http://127.0.0.1:8000/chat" `
  -Method POST `
  -Headers @{ "Content-Type" = "application/json" } `
  -Body '{"lead_id":"123","message":"What is your pricing?"}'

```


---
